{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b50f96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "import glob\n",
    "from matplotlib.image import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50a2c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [plt.imread(image) for image in glob.glob('test_images/*.jpg')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270b5cd",
   "metadata": {},
   "source": [
    "# Camera Calibration\n",
    "We start by preparing \"object points\", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here we are assuming the chessboard is fixed on the   (x, y) plane at z=0, such that the object points are the same for each calibration image. Thus, objp is just a replicated array of coordinates, and objpoints will be appended with a copy of it every time I successfully detect all chessboard corners in a test image. imgpoints will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.\n",
    "\n",
    "We then used the output objpoints and imgpoints to compute the camera calibration and distortion coefficients using the cv2.calibrateCamera() function. I applied this distortion correction to the test image using the cv2.undistort() function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9913e9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Calibrating Camera\n",
      "Camera successfully calibrated.\n"
     ]
    }
   ],
   "source": [
    "def calibrate_camera():\n",
    "    \n",
    "    \"\"\"\n",
    "        Calibrate camera from the provided calibration images\n",
    "    \"\"\"\n",
    "    nx = 9\n",
    "    ny = 6\n",
    "    images = glob.glob('camera_cal/calibration*.jpg')\n",
    "    \n",
    "    print(\"Starting Calibrating Camera\")\n",
    "    # Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((nx*ny,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nx,0:ny].T.reshape(-1, 2)\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "    # Apply findChessboardCorners to the individual images\n",
    "    for fname in images:    \n",
    "        img = mpimg.imread(fname)\n",
    "        image_shape = img.shape\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "    # Camera calibration, given object points, image points, and the shape of the grayscale image\n",
    "    if (len(objpoints) > 0):\n",
    "        # Camera successfully calibrated.\n",
    "        print(\"Camera successfully calibrated.\")\n",
    "        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, image_shape[:2], None, None)\n",
    "    else:\n",
    "        # Unable to calibrate the camera.\n",
    "        print(\"Unable to calibrate the camera.\")\n",
    "        ret, mtx, dist, rvecs, tvecs = (None, None, None, None, None)\n",
    "\n",
    "    return mtx, dist\n",
    "\n",
    "mtx, dist = calibrate_camera()\n",
    "    \n",
    "def undistort(image):\n",
    "    \"\"\"\n",
    "        Remove the distortion of an image \n",
    "    \"\"\"\n",
    "    return cv2.undistort(image, mtx, dist, None, mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee0d8c",
   "metadata": {},
   "source": [
    "# Showing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "260fe23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, cmap=None):\n",
    "    \"\"\"\n",
    "      show list of images  \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(40,40))    \n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(math.ceil(len(images) / 2), 2, i+1)\n",
    "        plt.imshow(image, cmap)\n",
    "        plt.autoscale(tight=True)\n",
    "    plt.show()\n",
    "    \n",
    "# show_images(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b2016",
   "metadata": {},
   "source": [
    "# Applying The sent function to a list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b5041b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(fun, images):\n",
    "    \"\"\"\n",
    "        apply function to list of images\n",
    "    \"\"\"\n",
    "    return list(map(fun, images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdccc74",
   "metadata": {},
   "source": [
    "# Converting Images to HLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c81fdbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_hls(image):\n",
    "    \"\"\"\n",
    "        Convert RGB image to HLS\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "\n",
    "def convert_to_gray(image):\n",
    "    \"\"\"\n",
    "        Convert RGB image to grayscale\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "def convert_to_rgp(image):\n",
    "    \"\"\"\n",
    "        Convert RGB image to grayscale\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "gray_images = apply(convert_to_gray, test_images)\n",
    "# show_images(gray_images, \"gray\")\n",
    "hls_images = apply(convert_to_hls, test_images)\n",
    "# show_images(hls_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f598f",
   "metadata": {},
   "source": [
    "# Extracting Yellow and White colors from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2f6b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hls_color_threshold(image):\n",
    "    \"\"\"\n",
    "        Extract yellow and white colors from image\n",
    "    \"\"\"\n",
    "    # Convert the input image to HLS\n",
    "    converted_image = convert_to_hls(image)\n",
    "    \n",
    "    # White color mask\n",
    "    lower_threshold = np.uint8([0, 190, 0])\n",
    "    upper_threshold = np.uint8([255, 255, 255])\n",
    "    white_mask = cv2.inRange(converted_image, lower_threshold, upper_threshold)\n",
    "    \n",
    "    # Yellow color mask\n",
    "    lower_threshold = np.uint8([10, 0, 90])\n",
    "    upper_threshold = np.uint8([50, 255, 255])\n",
    "    yellow_mask = cv2.inRange(converted_image, lower_threshold, upper_threshold)\n",
    "    \n",
    "    # Combine white and yellow masks\n",
    "    mask = cv2.bitwise_or(white_mask, yellow_mask)\n",
    "    masked_image = cv2.bitwise_and(image, image, mask = mask)\n",
    "    \n",
    "    return masked_image\n",
    "\n",
    "hls_color_threshold_images = apply(hls_color_threshold, test_images)\n",
    "# show_images(hls_color_threshold_images, \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929cdcc",
   "metadata": {},
   "source": [
    "# Applying Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bf845a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_thresholds(image):\n",
    "    \"\"\"\n",
    "        apply all the thresholds on image\n",
    "    \"\"\"\n",
    "    hls = hls_color_threshold(image)\n",
    "    gray = hls[:, :, 1]\n",
    "    s_channel = hls[:, :, 2]\n",
    "    \n",
    "    sobel_binary = np.zeros(shape=gray.shape, dtype=bool)\n",
    "    s_binary = sobel_binary\n",
    "    combined_binary = s_binary.astype(np.float32)\n",
    "\n",
    "    # Sobel Transform\n",
    "    sobel_kernel=7 \n",
    "    mag_thresh=(3, 255)\n",
    "    s_thresh=(170, 255)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = 0 #cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "\n",
    "    sobel_abs = np.abs(sobelx**2 + sobely**2)\n",
    "    sobel_abs = np.uint8(255 * sobel_abs / np.max(sobel_abs))\n",
    "\n",
    "    sobel_binary[(sobel_abs > mag_thresh[0]) & (sobel_abs <= mag_thresh[1])] = 1\n",
    "\n",
    "    # Threshold color channel\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "\n",
    "    # Combine the two binary thresholds\n",
    "\n",
    "    combined_binary[(s_binary == 1) | (sobel_binary == 1)] = 1\n",
    "    combined_binary = np.uint8(255 * combined_binary / np.max(combined_binary))\n",
    "    \n",
    "    return combined_binary\n",
    "\n",
    "threshold_images = apply(apply_thresholds, test_images)\n",
    "# show_images(threshold_images, \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf232e2f",
   "metadata": {},
   "source": [
    "# Region of Interest Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "187d1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_region_of_interest(image):\n",
    "    x = int(image.shape[1])\n",
    "    y = int(image.shape[0])\n",
    "    shape = np.array([[0, y], [x, y], [int(0.55 * x), int(0.6 * y)], [int(0.45 * x), int(0.6 * y)]])\n",
    "\n",
    "    # define a numpy array with the dimensions of image, but comprised of zeros\n",
    "    mask = np.zeros_like(image)\n",
    "\n",
    "    # Uses 3 channels or 1 channel for color depending on input image\n",
    "    if len(image.shape) > 2:\n",
    "        channel_count = image.shape[2]\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "\n",
    "    # creates a polygon with the mask color\n",
    "    cv2.fillPoly(mask, np.int32([shape]), ignore_mask_color)\n",
    "\n",
    "    # returns the image only where the mask pixels are not zero\n",
    "    masked_image = cv2.bitwise_and(image, mask)\n",
    "    return masked_image\n",
    "\n",
    "roi_images = apply(extract_region_of_interest, threshold_images)\n",
    "# show_images(roi_images, \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8ad02",
   "metadata": {},
   "source": [
    "# Perspective Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84627b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp(img):\n",
    "    # Vertices extracted manually for performing a perspective transform\n",
    "    bottom_left = [220,720]\n",
    "    bottom_right = [1110, 720]\n",
    "    top_left = [570, 470]\n",
    "    top_right = [722, 470]\n",
    "\n",
    "    source = np.float32([bottom_left,bottom_right,top_right,top_left])\n",
    "\n",
    "    pts = np.array([bottom_left,bottom_right,top_right,top_left], np.int32)\n",
    "    pts = pts.reshape((-1,1,2))\n",
    "    copy = img.copy()\n",
    "    cv2.polylines(copy,[pts],True,(255,0,0), thickness=3)\n",
    "\n",
    "    # Destination points are chosen such that straight lanes appear more or less parallel in the transformed image.\n",
    "    bottom_left = [320,720]\n",
    "    bottom_right = [920, 720]\n",
    "    top_left = [320, 1]\n",
    "    top_right = [920, 1]\n",
    "\n",
    "    dst = np.float32([bottom_left,bottom_right,top_right,top_left])\n",
    "    M = cv2.getPerspectiveTransform(source, dst)\n",
    "    M_inv = cv2.getPerspectiveTransform(dst, source)\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "\n",
    "    warped = cv2.warpPerspective(img, M, img_size , flags=cv2.INTER_LINEAR)\n",
    "    return warped, M_inv\n",
    "        \n",
    "warped_images = apply(warp, roi_images)\n",
    "warped_images = [warped_image[0] for warped_image in warped_images]\n",
    "# show_images(warped_images, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb0adf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ym_per_pix = 30 / 720.  # meters per pixel in y dimension\n",
    "xm_per_pix = 3.7 / 700  # meters per pixel in x dimension\n",
    "\n",
    "left_fit, right_fit, mov_avg_left, mov_avg_right = [], [], [], []\n",
    "def detect_lanes(img_w):\n",
    "    MOV_AVG_LENGTH = 12\n",
    "    global left_fit, right_fit, mov_avg_left, mov_avg_right\n",
    "    try:\n",
    "        left_fit, right_fit = fit_from_lines(left_fit, right_fit, img_w)\n",
    "\n",
    "        mov_avg_left = np.append(mov_avg_left,np.array([left_fit]), axis=0)\n",
    "        mov_avg_right = np.append(mov_avg_right,np.array([right_fit]), axis=0)\n",
    "\n",
    "    except:\n",
    "        left_fit, right_fit = sliding_windown(img_w)\n",
    "\n",
    "        mov_avg_left = np.array([left_fit])\n",
    "        mov_avg_right = np.array([right_fit])\n",
    "\n",
    "    left_fit = np.array([np.mean(mov_avg_left[::-1][:,0][0:MOV_AVG_LENGTH]),\n",
    "                         np.mean(mov_avg_left[::-1][:,1][0:MOV_AVG_LENGTH]),\n",
    "                         np.mean(mov_avg_left[::-1][:,2][0:MOV_AVG_LENGTH])])\n",
    "    right_fit = np.array([np.mean(mov_avg_right[::-1][:,0][0:MOV_AVG_LENGTH]),\n",
    "                         np.mean(mov_avg_right[::-1][:,1][0:MOV_AVG_LENGTH]),\n",
    "                         np.mean(mov_avg_right[::-1][:,2][0:MOV_AVG_LENGTH])])\n",
    "\n",
    "    if mov_avg_left.shape[0] > 1000:\n",
    "        mov_avg_left = mov_avg_left[0:MOV_AVG_LENGTH]\n",
    "    if mov_avg_right.shape[0] > 1000:\n",
    "        mov_avg_right = mov_avg_right[0:MOV_AVG_LENGTH]\n",
    "    return draw_lines(img_w, left_fit, right_fit)\n",
    "\n",
    "def sliding_windown(img_w):\n",
    "    histogram = np.sum(img_w[int(img_w.shape[0] / 2):, :], axis=0)\n",
    "    # Create an output image to draw on and visualize the result\n",
    "    out_img = np.dstack((img_w, img_w, img_w)) * 255\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0] / 2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set height of windows\n",
    "    window_height = np.int(img_w.shape[0] / nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = img_w.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = img_w.shape[0] - (window + 1) * window_height\n",
    "        win_y_high = img_w.shape[0] - window * window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img, (win_xleft_low, win_y_low), (win_xleft_high, win_y_high), (0, 255, 0), 2)\n",
    "        cv2.rectangle(out_img, (win_xright_low, win_y_low), (win_xright_high, win_y_high), (0, 255, 0), 2)\n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (\n",
    "            nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (\n",
    "            nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:\n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds]\n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    return left_fit, right_fit\n",
    "\n",
    "def fit_from_lines(left_fit, right_fit, img_w):\n",
    "    # Assume you now have a new warped binary image\n",
    "    # from the next frame of video (also called \"binary_warped\")\n",
    "    # It's now much easier to find line pixels!\n",
    "    nonzero = img_w.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    margin = 100\n",
    "    left_lane_inds = ((nonzerox > (left_fit[0] * (nonzeroy ** 2) + left_fit[1] * nonzeroy + left_fit[2] - margin)) & (\n",
    "    nonzerox < (left_fit[0] * (nonzeroy ** 2) + left_fit[1] * nonzeroy + left_fit[2] + margin)))\n",
    "    right_lane_inds = (\n",
    "    (nonzerox > (right_fit[0] * (nonzeroy ** 2) + right_fit[1] * nonzeroy + right_fit[2] - margin)) & (\n",
    "    nonzerox < (right_fit[0] * (nonzeroy ** 2) + right_fit[1] * nonzeroy + right_fit[2] + margin)))\n",
    "\n",
    "    # Again, extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds]\n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    return left_fit, right_fit\n",
    "\n",
    "\n",
    "def draw_lines(img, left_fit, right_fit):\n",
    "    # Create an image to draw the lines on\n",
    "    zero_image = np.zeros_like(img).astype(np.uint8)\n",
    "    result = np.dstack((zero_image, zero_image, zero_image))\n",
    "\n",
    "    ploty = np.linspace(0, img.shape[0] - 1, img.shape[0])\n",
    "\n",
    "    left_fitx = left_fit[0] * ploty ** 2 + left_fit[1] * ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0] * ploty ** 2 + right_fit[1] * ploty + right_fit[2]\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto blank image\n",
    "    cv2.fillPoly(result, np.int_([pts]),(0,0,255))\n",
    "\n",
    "    result_lines = np.dstack((zero_image, zero_image, zero_image))\n",
    "    cv2.polylines(result_lines, np.int_([pts_right]), isClosed=False, color=(255, 255, 0), thickness=25)\n",
    "    cv2.polylines(result_lines, np.int_([pts_left]), isClosed=False, color=(255, 255, 0), thickness=25)\n",
    "\n",
    "    result = cv2.addWeighted(result, 1, result_lines, 1, 0)\n",
    "    \n",
    "    radius = calculate_radius(img.shape[0], left_fitx, right_fitx)\n",
    "    off_center = calculate_off_center_postion(img.shape[0], left_fit[2], right_fit[2])\n",
    "    return result, radius, off_center\n",
    "\n",
    "   \n",
    "def calculate_radius(img_height, left_fitx, right_fitx):\n",
    "     # ----- Radius Calculation ------ #\n",
    "#     img_height = img.shape[0]\n",
    "    y_eval = img_height\n",
    "\n",
    "    ploty = np.linspace(0, img_height - 1, img_height)\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    left_fit_cr = np.polyfit(ploty * ym_per_pix, left_fitx * xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(ploty * ym_per_pix, right_fitx * xm_per_pix, 2)\n",
    "\n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2 * left_fit_cr[0] * y_eval * ym_per_pix + left_fit_cr[1]) ** 2) ** 1.5) / np.absolute(\n",
    "        2 * left_fit_cr[0])\n",
    "\n",
    "    right_curverad = ((1 + (2 * right_fit_cr[0] * y_eval * ym_per_pix + right_fit_cr[1]) ** 2) ** 1.5) / np.absolute(\n",
    "        2 * right_fit_cr[0])\n",
    "\n",
    "    radius = round((float(left_curverad) + float(right_curverad))/2.,2)\n",
    "    \n",
    "    return radius\n",
    "\n",
    "def calculate_off_center_postion(img_width, left_fit_x, right_fit_x):\n",
    "    # ----- Off Center Calculation ------ #\n",
    "\n",
    "    lane_width = (right_fit_x - left_fit_x) * xm_per_pix\n",
    "    center = (right_fit_x - left_fit_x) / 2\n",
    "    off_left = (center - left_fit_x) * xm_per_pix\n",
    "    off_right = -(right_fit_x - center) * xm_per_pix\n",
    "    off_center = round((center - img_width / 2.) * xm_per_pix,2)\n",
    "\n",
    "    return off_center\n",
    "        \n",
    "lanes_images = apply(detect_lanes, warped_images)\n",
    "lanes_images = [lanes_image[0] for lanes_image in lanes_images]\n",
    "# show_images(lanes_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d793f81",
   "metadata": {},
   "source": [
    "# Blending(Adding) Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d05d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_images(image_1, image_2, α = 1, β = 0.8, λ = 0):\n",
    "    \"\"\"\n",
    "        image_1 * α + image_2 * β + λ\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(image_1, α, image_2, β, λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497babb3",
   "metadata": {},
   "source": [
    "# Horizontal and Vertical Concatenation of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5103f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hconcat_resize(img_list, interpolation = cv2.INTER_CUBIC):\n",
    "    # take minimum hights\n",
    "    h_min = min(img.shape[0] for img in img_list)\n",
    "      \n",
    "    # image resizing \n",
    "    im_list_resize = [cv2.resize(img, (int(img.shape[1] * h_min / img.shape[0]),h_min), interpolation = interpolation) for img in img_list]\n",
    "      \n",
    "    # return final image\n",
    "    return cv2.hconcat(im_list_resize)\n",
    "\n",
    "def vconcat_resize(img_list, interpolation = cv2.INTER_CUBIC):\n",
    "    # take minimum width\n",
    "    w_min = min(img.shape[1] for img in img_list)\n",
    "      \n",
    "    # resizing images\n",
    "    im_list_resize = [cv2.resize(img,(w_min, int(img.shape[0] * w_min / img.shape[1])),interpolation = interpolation) for img in img_list]\n",
    "    # return final image\n",
    "    return cv2.vconcat(im_list_resize)\n",
    "\n",
    "def make_grid(list_2d, interpolation = cv2.INTER_CUBIC):\n",
    "    # function calling for every \n",
    "    # list of images\n",
    "    img_list_v = [hconcat_resize(list_h, interpolation = cv2.INTER_CUBIC) for list_h in list_2d]\n",
    "      \n",
    "    # return final image\n",
    "    return vconcat_resize(img_list_v, interpolation=cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88448196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_info(image, radius, offset):\n",
    "    # --- Print text on image ------ #\n",
    "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_color = (255, 255, 255)\n",
    "    font_size = 1.1\n",
    "    pos_x = 50\n",
    "    thickness = 2\n",
    "    \n",
    "    text = []\n",
    "    text.append(f\"Radius of curvature = {radius}m\")\n",
    "    text.append(f\"Vehicle position off center = {offset}m\")\n",
    "    \n",
    "    \n",
    "    for i, line in enumerate(text):\n",
    "        pos_y = pos_x + i * 50\n",
    "        cv2.putText(image, line, (pos_x, pos_y), font_face, font_size, font_color, thickness, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d111dfc",
   "metadata": {},
   "source": [
    "# Main Pipeline of Lane Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86b95a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_lanes_image, pre_radius, pre_offset = 0, 0, 0\n",
    "\n",
    "def pipeline(image, debug_mode):\n",
    "    global pre_lanes_image, pre_radius, pre_offset\n",
    "    undistorted_image = undistort(image)\n",
    "    threshold_image = apply_thresholds(undistorted_image)\n",
    "    roi_image = extract_region_of_interest(threshold_image)\n",
    "    warped, M_inv = warp(roi_image)\n",
    "    try:\n",
    "        lanes_image, radius, offset = detect_lanes(warped)\n",
    "        pre_lanes_image, pre_radius, pre_offset = lanes_image, radius, offset\n",
    "       \n",
    "    except:\n",
    "         lanes_image, radius, offset = pre_lanes_image, pre_radius, pre_offset\n",
    "    \n",
    "    newwarp = cv2.warpPerspective(lanes_image, M_inv, (lanes_image.shape[1], lanes_image.shape[0])) \n",
    "    final_image = blend_images(newwarp, image)\n",
    "    show_info(final_image, radius, offset)\n",
    "\n",
    "    if debug_mode:\n",
    "        return make_grid(\n",
    "            [[final_image],\n",
    "             [undistorted_image, convert_to_rgp(threshold_image), convert_to_rgp(roi_image)],\n",
    "             [convert_to_rgp(warped), lanes_image, newwarp], \n",
    "             ])\n",
    "    return final_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8037d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_videos/challenge_video.mp4.\n",
      "Moviepy - Writing video output_videos/challenge_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos/challenge_video.mp4\n"
     ]
    }
   ],
   "source": [
    "PYTHONFILE = False\n",
    "\n",
    "input_video = \"test_videos/challenge_video.mp4\"\n",
    "if PYTHONFILE and len(sys.argv) > 1:\n",
    "    input_video = sys.argv[1]\n",
    "\n",
    "debug = True\n",
    "if PYTHONFILE and len(sys.argv) > 2 and sys.argv[2] == \"debug\":\n",
    "    debug = True\n",
    "\n",
    "output_video = \"output_videos/\" + input_video.split(\"/\")[-1]\n",
    "\n",
    "clip = VideoFileClip(input_video)\n",
    "clip.fl_image(lambda image: pipeline(image, debug)).write_videofile(output_video, audio=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
